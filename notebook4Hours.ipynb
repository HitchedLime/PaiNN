{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02456 Molecular Property Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic example of how to train the PaiNN model to predict the QM9 property\n",
    "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
    "properties) is computed as a sum of atomic contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 7])\n",
      "2.5.1+cu118\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_scatter import scatter\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "index = torch.tensor([0, 0, 1, 1])  # Group indices\n",
    "result = scatter(x, index, dim=0, reduce=\"sum\")\n",
    "print(result)  # Outp\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)  # PyTorch version\n",
    "print(torch.version.cuda)  # CUDA version (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch_scatter import scatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QM9 Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import Optional, List, Union, Tuple\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "\n",
    "class GetTarget(BaseTransform):\n",
    "    def __init__(self, target: Optional[int] = None) -> None:\n",
    "        self.target = [target]\n",
    "\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        if self.target is not None:\n",
    "            data.y = data.y[:, self.target]\n",
    "        return data\n",
    "\n",
    "\n",
    "class QM9DataModule(pl.LightningDataModule):\n",
    "\n",
    "    target_types = ['atomwise' for _ in range(19)]\n",
    "    target_types[0] = 'dipole_moment'\n",
    "    target_types[5] = 'electronic_spatial_extent'\n",
    "\n",
    "    # Specify unit conversions (eV to meV).\n",
    "    unit_conversion = {\n",
    "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
    "        else (lambda t: t)\n",
    "        for i in range(19)\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: int = 7,\n",
    "        data_dir: str = 'data/',\n",
    "        batch_size_train: int = 100,\n",
    "        batch_size_inference: int = 1000,\n",
    "        num_workers: int = 0,\n",
    "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
    "        seed: int = 0,\n",
    "        subset_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.batch_size_inference = batch_size_inference\n",
    "        self.num_workers = num_workers\n",
    "        self.splits = splits\n",
    "        self.seed = seed\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download data\n",
    "        QM9(root=self.data_dir)\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        dataset = dataset[rng.permutation(len(dataset))]\n",
    "\n",
    "        # Subset dataset\n",
    "        if self.subset_size is not None:\n",
    "            dataset = dataset[:self.subset_size]\n",
    "        \n",
    "        # Split dataset\n",
    "        if all([type(split) == int for split in self.splits]):\n",
    "            split_sizes = self.splits\n",
    "        elif all([type(split) == float for split in self.splits]):\n",
    "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
    "\n",
    "        split_idx = np.cumsum(split_sizes)\n",
    "        self.data_train = dataset[:split_idx[0]]\n",
    "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
    "        self.data_test = dataset[split_idx[1]:]\n",
    "\n",
    "\n",
    "    def get_target_stats(\n",
    "        self,\n",
    "        remove_atom_refs: bool = True,\n",
    "        divide_by_atoms: bool = True\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        atom_refs = self.data_train.atomref(self.target)\n",
    "\n",
    "        ys = list()\n",
    "        for batch in self.train_dataloader(shuffle=False):\n",
    "            y = batch.y.clone()\n",
    "            if remove_atom_refs and atom_refs is not None:\n",
    "                y.index_add_(\n",
    "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
    "                )\n",
    "            if divide_by_atoms:\n",
    "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
    "                y = y / num_atoms.unsqueeze(-1)\n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.cat(ys, dim=0)\n",
    "        return y.mean(), y.std(), atom_refs\n",
    "\n",
    "\n",
    "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.batch_size_train,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valid values for target [0,18] where \n",
    "<br>\n",
    "0: Dipole moment (μ). <br>\n",
    "1: Isotropic polarizability (α). <br>\n",
    "2: HOMO (highest occupied molecular orbital) energy. <br>\n",
    "3: LUMO (lowest unoccupied molecular orbital) energy. <br>\n",
    "4: Gap between HOMO and LUMO. <br>\n",
    "... <br>\n",
    "18: Internal energy (U) at 298.15K. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 1], pos=[5, 3], idx=[1], name='gdb_1', z=[5])\n",
      "QM9(130831)\n"
     ]
    }
   ],
   "source": [
    "dataset = QM9(root='data/', transform=GetTarget(target=7))\n",
    "print(dataset[0])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. x=[5, 11]\n",
    "Description: This is the node feature matrix. It contains features for each node in the graph (each atom in the molecule).\n",
    "The shape [5, 11] means there are 5 nodes (atoms) in this molecule, and each atom has a feature vector of length 11. These features could include various atomic properties such as atomic number, hybridization, etc.\n",
    "Example: If the molecule has 5 atoms, each atom's feature could represent properties like charge, hybridization state, or other descriptors.\n",
    "\n",
    "2. edge_index=[2, 8]\n",
    "Description: This is the edge index that represents the connectivity (bonds) between atoms. It's a sparse matrix in COO (Coordinate) format, where each column represents a directed edge between two atoms.\n",
    "The shape [2, 8] means there are 8 edges, and each edge is represented by two values (source, target) for each direction. The 2 indicates there are 2 rows (one for the source atom and one for the target atom), and 8 indicates there are 8 edges.\n",
    "Example: For a molecule with 5 atoms, the edge_index matrix could look like this:\n",
    "\n",
    "edge_index = [ 0 0 1 2 2 3 3 4 \n",
    "               1 2 3 3 4 4 0 1 ]\n",
    "\n",
    "3. edge_attr=[8, 4]\n",
    "Description: This is the edge attribute matrix, which stores additional information about each edge (bond). The shape [8, 4] means there are 8 edges, and each edge has a feature vector of length 4.\n",
    "These attributes could represent things like bond type (single, double, triple), bond length, or other chemical properties associated with the bond.\n",
    "Example: For 8 edges, each with 4 attributes, this could be a matrix representing things like bond length, bond type, or bond angle.\n",
    "\n",
    "4. y=[1, 1]\n",
    "Description: This is the target value for the molecular property that is being predicted. The shape [1, 1] indicates that there is a single target property for the entire graph (molecule).\n",
    "Since you applied the GetTarget(target=7) transform, this corresponds to the 7th property in the QM9 dataset (e.g., y could represent a target like the molecular dipole moment or energy, depending on the dataset).\n",
    "Example: If you were predicting the dipole moment, y would store the dipole moment value for the molecule.\n",
    "\n",
    "5. pos=[5, 3]\n",
    "Description: This is the position matrix for each node (atom) in the graph. The shape [5, 3] means there are 5 atoms, and each atom has a 3D coordinate (x, y, z).\n",
    "This attribute is typically used in molecular graph representations where the spatial coordinates of atoms are important for geometric properties, especially for 3D molecule-related tasks.\n",
    "Example: For each atom in the molecule, pos could represent its 3D position in space (e.g., pos[0] could be [x_0, y_0, z_0] for atom 0).\n",
    "\n",
    "6. idx=[1]\n",
    "Description: This is a unique identifier for the molecule in the dataset. The shape [1] indicates there is one molecule, and the value 1 is a placeholder index (could represent a unique molecule ID like gdb_1). This helps in tracking different molecules in the dataset.\n",
    "\n",
    "7. name='gdb_1'\n",
    "Description: This is the name or ID of the molecule in the dataset. In this case, 'gdb_1' indicates that this molecule is the first molecule in the dataset, corresponding to the name gdb_1 in the QM9 dataset. Example: This could be used for logging or identifying which molecule is being processed.\n",
    "\n",
    "8. z=[5]\n",
    "Description: This represents the atomic numbers of the atoms in the graph. The shape [5] means there are 5 atoms, and the list [5] corresponds to the atomic numbers of the atoms (for example, 5 could represent boron if all atoms are of type boron). Example: If this is a molecule of boron (B), the array could look like [5, 5, 5, 5, 5] for each boron atom.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and use the data. 'train_loader' is batched with 100 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[1791, 11], edge_index=[2, 3744], edge_attr=[3744, 4], y=[100, 1], pos=[1791, 3], idx=[100], name=[100], z=[1791], batch=[1791], ptr=[101])\n"
     ]
    }
   ],
   "source": [
    "data_module = QM9DataModule(target=7)\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scalar message function\n",
    "\n",
    "#### 1. Feature Vector \n",
    "\n",
    "\\begin{align*}\n",
    "    x \\in \\mathbb{R}^{F\\times1}\n",
    "\\end{align*}\n",
    "\n",
    "* E.g., \n",
    "\n",
    "\\begin{align*}\n",
    "    x_A = [ 0.5, 1.2 ], x_B = [ 0.8, 0.9 ]\n",
    "\\end{align*}\n",
    "\n",
    "#### 2. Coordinates\n",
    "\n",
    "\\begin{align*}\n",
    "    \\vec{r} \\in \\mathbb{R}^{ 1 \\times 3 }\n",
    "\\end{align*}\n",
    "\n",
    "* E.g., \n",
    "\n",
    "\\begin{align*}\n",
    "    \\vec{r}_A = [ 1.0, 0.0 0.0 ], \\ \\vec{r}_B = [ 0.0, 1.0 0.0 ]\n",
    "\\end{align*}\n",
    "\n",
    "#### 4. Vectorial features:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\vec{x} \\in \\mathbb{R}^{ F \\times 3 }\n",
    "\\end{align*}\n",
    "\n",
    "* E.g., \n",
    "\n",
    "\\begin{align*}\n",
    "    \\vec{x}_A = \n",
    "    \\begin{bmatrix}\n",
    "    1.0 & 0.0 & 0.0 \\\\\n",
    "    0.0 & 1.0 & 0.0\n",
    "    \\end{bmatrix}\n",
    "    , \\ \n",
    "    \\vec{x}_ = \n",
    "    \\begin{bmatrix}\n",
    "    0.5 & 0.0 & 0.5 \\\\\n",
    "    0.0 & 0.5 & 1.0\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is for predicting the molecuel after processed by a PaiNN model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AtomwisePostProcessing(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-processing for (QM9) properties that are predicted as sums of atomic\n",
    "    contributions.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs: int,\n",
    "        mean: torch.FloatTensor,\n",
    "        std: torch.FloatTensor,\n",
    "        atom_refs: torch.FloatTensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_outputs: Integer with the number of model outputs. In most\n",
    "                cases 1.\n",
    "            mean: torch.FloatTensor with mean value to shift atomwise\n",
    "                contributions by.\n",
    "            std: torch.FloatTensor with standard deviation to scale atomwise\n",
    "                contributions by.\n",
    "            atom_refs: torch.FloatTensor of size [num_atom_types, 1] with\n",
    "                atomic reference values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.register_buffer('scale', std)\n",
    "        self.register_buffer('shift', mean)\n",
    "        self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_contributions: torch.FloatTensor,\n",
    "        atoms: torch.LongTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Atomwise post-processing operations and atomic sum.\n",
    "\n",
    "        Args:\n",
    "            atomic_contributions: torch.FloatTensor of size [num_nodes,\n",
    "                num_outputs] with each node's contribution to the overall graph\n",
    "                prediction, i.e., each atom's contribution to the overall\n",
    "                molecular property prediction.\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FLoatTensor of size [num_graphs, num_outputs] with\n",
    "            predictions for each graph (molecule).\n",
    "        \"\"\"\n",
    "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
    "\n",
    "        atomic_contributions = atomic_contributions*self.scale + self.shift\n",
    "        atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
    "\n",
    "        # Sum contributions for each graph\n",
    "        output_per_graph = torch.zeros(\n",
    "            (num_graphs, self.num_outputs),\n",
    "            device=atomic_contributions.device,\n",
    "        )\n",
    "        output_per_graph.index_add_(\n",
    "            dim=0,\n",
    "            index=graph_indexes,\n",
    "            source=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        return output_per_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtomwisePostProcessing(\n",
      "  (atom_refs): Embedding(5, 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example data\n",
    "num_atom_types = 5\n",
    "num_outputs = 1\n",
    "mean = torch.tensor([0.5])  # Example mean\n",
    "std = torch.tensor([2.0])   # Example standard deviation\n",
    "atom_refs = torch.tensor([[0.1], [0.2], [0.3], [0.4], [0.5]])  # Reference values\n",
    "\n",
    "# Initialize the post-processing module\n",
    "post_processor = AtomwisePostProcessing(\n",
    "    num_outputs=num_outputs,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    atom_refs=atom_refs,\n",
    ")\n",
    "print(post_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000],\n",
      "        [0.2000],\n",
      "        [0.3000],\n",
      "        [0.4000]])\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0, 0, 1, 1])\n",
      "tensor([[1.9000],\n",
      "        [3.1000]])\n"
     ]
    }
   ],
   "source": [
    "# Atomic contributions for nodes\n",
    "atomic_contributions = torch.tensor([\n",
    "    [0.1],  # Atom 0\n",
    "    [0.2],  # Atom 1\n",
    "    [0.3],  # Atom 2\n",
    "    [0.4],  # Atom 3\n",
    "])\n",
    "print(atomic_contributions)\n",
    "\n",
    "# Atom types (corresponding to atom_refs)\n",
    "atoms = torch.tensor([0, 1, 2, 3])  # 4 atoms, each with a specific type\n",
    "print(atoms)\n",
    "# Graph indexes for each atom\n",
    "graph_indexes = torch.tensor([0, 0, 1, 1])  # First two atoms in graph 0, last two in graph 1\n",
    "print(graph_indexes)\n",
    "\n",
    "output = post_processor(\n",
    "    atomic_contributions=atomic_contributions,\n",
    "    atoms=atoms,\n",
    "    graph_indexes=graph_indexes,\n",
    ")\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Compute Scala Messages\n",
    "\n",
    "\\begin{align*}\n",
    "    m_{ij} \\ \\text{or} \\ h^n  & = \\phi_m (x_i, \\ x_j, \\ || \\vec{d}_{ij} || ) = \\mathsf{MLP}( [ x_i, x_j, || \\vec{d}_{ij} || ]) \\\\\n",
    "           \\Rightarrow M_i & = \\sum_{j \\in \\mathcal{N}(i) } m_{ij} x_j \\cdot \\vec{d}_{ij} \\ \\text{ aggregation }\\\\\n",
    "           \\Rightarrow x' & =  \\phi_m (x_i, M_i ) \\ \\text{ update }  \\\\  \n",
    "           & = x_i + M_i \n",
    "\\end{align*}\n",
    "\n",
    "* e.g. \n",
    "\n",
    "\\begin{align*}\n",
    "    m_{A} & = \\phi (x_A, \\ x_B, || \\vec{d}_{AB} || ) \\\\\n",
    "           & = \\mathsf{MLP}([0.5, 1.2, 0.8, 0.9, \\sqrt{2} ]) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "##### Note. Displacement magnitude\n",
    "\n",
    "\\begin{align*}\n",
    "    || \\vec{d}_{AB} || & = \\vec{r}_{A} - \\vec{r}_{B} \\\\\n",
    "    & = \\sqrt{ (-1)^2 + (1)^2 + (0)^2 } || \\\\\n",
    "    & = \\sqrt{2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We'll start with the following setting for the MLP, 2 laye network, input size 5, hidden size 4 and ouput size 2.\n",
    "\n",
    "1.3 Linear layer(Linear compbination) \n",
    "\n",
    "* e.g., \n",
    "\\begin{align*}\n",
    "    h^n = w_n m_i + b_n\n",
    "\\end{align*}\n",
    "\n",
    "1.4. Initialize weights and biases, typically they are initalized radomly.\n",
    "\n",
    "e.g.,\n",
    "\n",
    "\\begin{align*}\n",
    "    h^1 = [0.996,0.802,−0.168,0.009]\n",
    "\\end{align*}\n",
    "\n",
    "1.5. Apply activation function SiLU\n",
    "\n",
    "    SiLU(h^1) = ?\n",
    "\n",
    "1.6 Apply SiLU(h^1) to next connected layer(s)\n",
    "e.g.\n",
    "\\begin{align*}\n",
    "    m_{AB} \\text{ or } (h^2) = w_2 \\text{}(h^1) + b_2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "2. Compute Vectorial Messages\n",
    "\n",
    "Vectorial messages are just matrix version of the function above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaiNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_cluster import radius_graph\n",
    "class PaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_message_passing_layers: Number of message passing layers in\n",
    "                the PaiNN model.\n",
    "            num_features: Size of the node embeddings (scalar features) and\n",
    "                vector features.\n",
    "            num_outputs: Number of model outputs. In most cases 1.\n",
    "            num_rbf_features: Number of radial basis functions to represent\n",
    "                distances.\n",
    "            num_unique_atoms: Number of unique atoms in the data that we want\n",
    "                to learn embeddings for.\n",
    "            cutoff_dist: Euclidean distance threshold for determining whether \n",
    "                two nodes (atoms) are neighbours.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #raise NotImplementedError\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.num_message_passing_layers = num_message_passing_layers\n",
    "        self.cutoff_dist = cutoff_dist\n",
    "\n",
    "        # Atom embedding layer\n",
    "        self.atom_embedding = nn.Embedding(num_unique_atoms, num_features)\n",
    "        \n",
    "        # Radial basis function (RBF) transformation\n",
    "        self.rbf_transform = nn.Sequential(\n",
    "            nn.Linear(num_rbf_features, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        # Message passing layers\n",
    "        self.message_passing_layers = nn.ModuleList([\n",
    "            # MessagePassingLayer(num_features, num_rbf_features)\n",
    "            MessagePassingLayer(num_features, num_rbf_features)\n",
    "            for _ in range(num_message_passing_layers)\n",
    "        ])\n",
    "\n",
    "        # Final readout layer\n",
    "        self.readout = nn.Sequential(\n",
    "            #nn.Linear(num_features, 64),\n",
    "            nn.Linear(num_features, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            #nn.Linear(64, num_outputs),\n",
    "            nn.Linear(64, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atoms: torch.LongTensor,\n",
    "        atom_positions: torch.FloatTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward pass of PaiNN. Includes the readout network highlighted in blue\n",
    "        in Figure 2 in (Schütt et al., 2021) with normal linear layers which is\n",
    "        used for predicting properties as sums of atomic contributions. The\n",
    "        post-processing and final sum is perfomed with\n",
    "        src.models.AtomwisePostProcessing.\n",
    "\n",
    "        Args:\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            atom_positions: torch.FloatTensor of size [num_nodes, 3] with\n",
    "                euclidean coordinates of each node / atom.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FloatTensor of size [num_nodes, num_outputs] with atomic\n",
    "            contributions to the overall molecular property prediction.\n",
    "        \"\"\"\n",
    "        #raise NotImplementedError\n",
    "        # Atom embeddings: Shape [num_nodes, num_features]\n",
    "        x = self.atom_embedding(atoms)\n",
    "        # print(f\"shape of the atom embeddings {x.shape}\")\n",
    "        # # # Pairwise distances\n",
    "        # # d_ij = torch.cdist(atom_positions, atom_positions)  # Shape [num_nodes, num_nodes]\n",
    "\n",
    "        # # Mask out distances beyond cutoff\n",
    "        # adjacency_mask = (d_ij <= self.cutoff_dist).float()\n",
    "        # print(f\"shape of the adjacency mask {adjacency_mask.shape}\")\n",
    "\n",
    "        # Compute sparse neighbor graph\n",
    "        edge_index = radius_graph(atom_positions, r=self.cutoff_dist, batch=graph_indexes)\n",
    "\n",
    "        # Sparse pairwise distances\n",
    "        d_ij_sparse = torch.norm(\n",
    "            atom_positions[edge_index[0]] - atom_positions[edge_index[1]], dim=-1\n",
    "        )\n",
    "        #print(f\"shape of the Sparse pairwise distances {d_ij_sparse.shape}\")\n",
    "        \n",
    "        # # Apply radial basis functions to distances\n",
    "        # rbf_features = radial_basis_function_transform(d_ij, self.cutoff_dist)  # Shape [num_nodes, num_nodes, num_rbf_features]\n",
    "        # print(f\"shape of the distance before rbf {rbf_features.shape}\")\n",
    "        # rbf_features = self.rbf_transform(rbf_features)  # Shape [num_nodes, num_nodes, num_features]\n",
    "        # print(f\"shape of the distance after rbf {rbf_features.shape}\")\n",
    "\n",
    "        # Radial basis function on sparse distances\n",
    "        rbf_features_sparse = radial_basis_function_transform(d_ij_sparse, self.cutoff_dist)\n",
    "        #print(f\"shape of the distance before rbf {rbf_features_sparse.shape}\")\n",
    "        rbf_features_sparse = self.rbf_transform(rbf_features_sparse)\n",
    "        #print(f\"shape of the distance after rbf {rbf_features_sparse.shape}\")\n",
    "\n",
    "        # # Message passing layers\n",
    "        # for layer in self.message_passing_layers:\n",
    "        #     x = layer(x, atom_positions, rbf_features, adjacency_mask)\n",
    "\n",
    "        # Message passing layers with sparse features\n",
    "        for layer in self.message_passing_layers:\n",
    "            x = layer(x, atom_positions, rbf_features_sparse, edge_index)\n",
    "\n",
    "        # Readout: Predict per-atom properties\n",
    "        atomic_predictions = self.readout(x)  # Shape [num_nodes, num_outputs]\n",
    "        #print(f\"shape of the atomic predictions {atomic_predictions.shape}\")\n",
    "        # Aggregate per-atom contributions for each graph\n",
    "        # molecular_predictions = torch.zeros(graph_indexes.max() + 1, self.readout[-1].out_features).to(x.device)\n",
    "        # molecular_predictions.index_add_(\n",
    "        #     0, graph_indexes, atomic_predictions\n",
    "        # )  # Aggregate contributions by graph index\n",
    "\n",
    "\n",
    "\n",
    "        #return molecular_predictions\n",
    "        return atomic_predictions\n",
    "\n",
    "\n",
    "# class MessagePassingLayer(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A single message-passing layer for PaiNN.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_features, num_rbf_features):\n",
    "#         super().__init__()\n",
    "#         self.linear = nn.Linear(num_features, num_features)\n",
    "#         self.vector_update = nn.Linear(num_features, num_features)\n",
    "#         self.scalar_update = nn.Linear(num_features, num_features)\n",
    "\n",
    "#     def forward(self, x, atom_positions, rbf_features, adjacency_mask):\n",
    "#         # Aggregate messages\n",
    "#         messages = self.aggregate_messages(x, rbf_features, adjacency_mask)\n",
    "\n",
    "#         # Update scalar features\n",
    "#         scalar_update = self.scalar_update(messages)\n",
    "#         x = x + scalar_update\n",
    "\n",
    "#         # Update vector features\n",
    "#         vector_update = self.vector_update(messages)\n",
    "#         x = x + vector_update\n",
    "\n",
    "#         print(f\"x shape: {x.shape}\")  # Shape of scalar features\n",
    "#         print(f\"scalar_update shape: {scalar_update.shape}\")  # Shape of scalar updates\n",
    "\n",
    "#         return x\n",
    "\n",
    "class MessagePassingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single message-passing layer for PaiNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features: int, num_rbf_features: int):\n",
    "        \"\"\"\n",
    "        Initialize the message-passing layer.\n",
    "\n",
    "        Args:\n",
    "            num_features: Dimensionality of the scalar and vector features.\n",
    "            num_rbf_features: Number of RBF-transformed features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "\n",
    "        # Learnable linear transformations\n",
    "        self.scalar_update = nn.Linear(num_features, num_features)\n",
    "        self.vector_update = nn.Linear(num_features, num_features)\n",
    "\n",
    "    def forward(self, x, atom_positions, rbf_features_sparse, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass for the message-passing layer.\n",
    "\n",
    "        Args:\n",
    "            x: Node embeddings (num_nodes, num_features).\n",
    "            atom_positions: Node positions (num_nodes, 3).\n",
    "            rbf_features_sparse: Sparse RBF features (num_edges, num_features).\n",
    "            edge_index: Sparse adjacency edges (2, num_edges).\n",
    "        \"\"\"\n",
    "        # Aggregate messages\n",
    "        messages = self.aggregate_messages(x, rbf_features_sparse, edge_index)\n",
    "\n",
    "        # Update scalar features\n",
    "        scalar_update = self.scalar_update(messages)\n",
    "        x = x + scalar_update\n",
    "\n",
    "        # Update vector features\n",
    "        vector_update = self.vector_update(messages)\n",
    "        x = x + vector_update\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    # def aggregate_messages(self, x, rbf_features, adjacency_mask):\n",
    "    #     # Use adjacency mask to aggregate weighted features\n",
    "    #     messages = adjacency_mask.unsqueeze(-1) * rbf_features\n",
    "    #     return torch.sum(messages, dim=1)\n",
    "\n",
    "    def aggregate_messages(self, x, rbf_features_sparse, edge_index):\n",
    "        \"\"\"\n",
    "        Aggregate messages for sparse adjacency.\n",
    "        Args:\n",
    "            x: Node embeddings (num_nodes, num_features).\n",
    "            rbf_features_sparse: RBF-transformed distance features (num_edges, num_features).\n",
    "            edge_index: Sparse adjacency edges (2, num_edges).\n",
    "        \"\"\"\n",
    "        # Gather source node features\n",
    "        source_node_features = x[edge_index[1]]  # Features for source nodes in edges\n",
    "\n",
    "        # Compute messages\n",
    "        messages = source_node_features * rbf_features_sparse  # Element-wise multiplication\n",
    "\n",
    "        # Aggregate messages to target nodes\n",
    "        aggregated_messages = torch.zeros_like(x).index_add_(\n",
    "            0, edge_index[0], messages\n",
    "        )  # Sum messages to target nodes\n",
    "\n",
    "        return aggregated_messages\n",
    "\n",
    "\n",
    "\n",
    "# def radial_basis_function_transform(distances, cutoff):\n",
    "#     \"\"\"\n",
    "#     Compute radial basis function features for distances.\n",
    "#     \"\"\"\n",
    "#     # Define RBF centers and width\n",
    "#     num_rbf_features = 20\n",
    "#     rbf_centers = torch.linspace(0, cutoff, num_rbf_features).to(distances.device)\n",
    "#     rbf_width = (rbf_centers[1] - rbf_centers[0])\n",
    "\n",
    "#     # Compute RBF values\n",
    "#     rbf_values = torch.exp(-((distances.unsqueeze(-1) - rbf_centers) ** 2) / (2 * rbf_width ** 2))\n",
    "#     return rbf_values\n",
    "\n",
    "\n",
    "def radial_basis_function_transform(distances, cutoff, num_rbf_features=20):\n",
    "    \"\"\"\n",
    "    Compute sinusoidal radial basis function features for distances.\n",
    "    \n",
    "    Parameters:\n",
    "        distances (torch.Tensor): Pairwise distances (1D tensor).\n",
    "        cutoff (float): Cutoff radius.\n",
    "        num_rbf_features (int): Number of RBF features.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: RBF-transformed features (shape: [len(distances), num_rbf_features]).\n",
    "    \"\"\"\n",
    "    # Define indices for the sinusoidal basis\n",
    "    n_values = torch.arange(1, num_rbf_features + 1, device=distances.device).float()\n",
    "\n",
    "    # Calculate RBF values\n",
    "    scaled_distances = distances.unsqueeze(-1)  # Shape: [N, 1]\n",
    "    sinusoidal_values = torch.sin(n_values * torch.pi * scaled_distances / cutoff) / (scaled_distances + 1e-8)\n",
    "\n",
    "    # Mask for values beyond the cutoff\n",
    "    mask = (distances <= cutoff).unsqueeze(-1)  # Shape: [N, 1]\n",
    "    rbf_values = sinusoidal_values * mask.float()\n",
    "\n",
    "    return rbf_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli(args: list = []):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0)\n",
    "\n",
    "    # Data\n",
    "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
    "    parser.add_argument('--data_dir', default='data/', type=str)\n",
    "    #parser.add_argument('--batch_size_train', default=100, type=int)\n",
    "    parser.add_argument('--batch_size_train', default=1000, type=int)\n",
    "    #parser.add_argument('--batch_size_train', default=10000, type=int)\n",
    "    #parser.add_argument('--batch_size_train', default=3000, type=int)\n",
    "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
    "    # parser.add_argument('--batch_size_train', default=30, type=int)\n",
    "    # parser.add_argument('--batch_size_inference', default=10, type=int)\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
    "    parser.add_argument('--subset_size', default=None, type=int)\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument('--num_message_passing_layers', default=3, type=int)\n",
    "    parser.add_argument('--num_features', default=128, type=int)\n",
    "    parser.add_argument('--num_outputs', default=1, type=int)\n",
    "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
    "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
    "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--num_epochs', default=1000, type=int)\n",
    "    #parser.add_argument('--num_epochs', default=10, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [4:16:03<00:00, 15.36s/it, Train loss: 9.313e-02]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 100.105\n"
     ]
    }
   ],
   "source": [
    "args = [] # Specify non-default arguments in this list\n",
    "args = cli(args)\n",
    "seed_everything(args.seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "dm = QM9DataModule(\n",
    "    target=args.target,\n",
    "    data_dir=args.data_dir,\n",
    "    batch_size_train=args.batch_size_train,\n",
    "    batch_size_inference=args.batch_size_inference,\n",
    "    num_workers=args.num_workers,\n",
    "    splits=args.splits,\n",
    "    seed=args.seed,\n",
    "    subset_size=args.subset_size,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "y_mean, y_std, atom_refs = dm.get_target_stats(\n",
    "    remove_atom_refs=True, divide_by_atoms=True\n",
    ")\n",
    "\n",
    "painn = PaiNN(\n",
    "    num_message_passing_layers=args.num_message_passing_layers,\n",
    "    num_features=args.num_features,\n",
    "    num_outputs=args.num_outputs, \n",
    "    num_rbf_features=args.num_rbf_features,\n",
    "    num_unique_atoms=args.num_unique_atoms,\n",
    "    cutoff_dist=args.cutoff_dist,\n",
    ").to(device)\n",
    "post_processing = AtomwisePostProcessing(\n",
    "    args.num_outputs, y_mean, y_std, atom_refs\n",
    ").to(device)\n",
    "\n",
    "# painn.to(device)\n",
    "# post_processing.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    painn.parameters(),\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "painn.train()\n",
    "pbar = trange(args.num_epochs)\n",
    "for epoch in pbar:\n",
    "\n",
    "    loss_epoch = 0.\n",
    "    for batch in dm.train_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch\n",
    "        )\n",
    "        #print(atomic_contributions)\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "        loss = loss_step / len(batch.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss_step.detach().item()\n",
    "    loss_epoch /= len(dm.data_train)\n",
    "    pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
    "\n",
    "mae = 0\n",
    "painn.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dm.test_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch,\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "mae /= len(dm.data_test)\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "print(f'Test MAE: {unit_conversion(mae):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.amp import autocast, GradScaler\n",
    "\n",
    "# # Initialize arguments and set device\n",
    "# args = []\n",
    "# args = cli(args)\n",
    "# seed_everything(args.seed)\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(device)\n",
    "\n",
    "# # Data Module setup\n",
    "# dm = QM9DataModule(\n",
    "#     target=args.target,\n",
    "#     data_dir=args.data_dir,\n",
    "#     batch_size_train=args.batch_size_train,\n",
    "#     batch_size_inference=args.batch_size_inference,\n",
    "#     num_workers=args.num_workers,\n",
    "#     splits=args.splits,\n",
    "#     seed=args.seed,\n",
    "#     subset_size=args.subset_size,\n",
    "# )\n",
    "# dm.prepare_data()\n",
    "# dm.setup()\n",
    "# #print(dm.train_dataloader())\n",
    "# y_mean, y_std, atom_refs = dm.get_target_stats(remove_atom_refs=True, divide_by_atoms=True)\n",
    "\n",
    "# # Model and Post-Processing setup\n",
    "# painn = PaiNN(\n",
    "#     num_message_passing_layers=args.num_message_passing_layers,\n",
    "#     num_features=args.num_features,\n",
    "#     num_outputs=args.num_outputs, \n",
    "#     num_rbf_features=args.num_rbf_features,\n",
    "#     num_unique_atoms=args.num_unique_atoms,\n",
    "#     cutoff_dist=args.cutoff_dist,\n",
    "# ).to(device)\n",
    "\n",
    "# post_processing = AtomwisePostProcessing(args.num_outputs, y_mean, y_std, atom_refs).to(device)\n",
    "\n",
    "# # Optimizer and Mixed Precision setup\n",
    "# optimizer = torch.optim.AdamW(painn.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "# scaler = GradScaler()  # for mixed precision\n",
    "\n",
    "# # Training loop\n",
    "# painn.train()\n",
    "# pbar = trange(args.num_epochs)\n",
    "# for epoch in pbar:\n",
    "#     loss_epoch = 0.0\n",
    "#     for batch in dm.train_dataloader():\n",
    "#         batch = batch.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Mixed precision training (autocast)\n",
    "#         with autocast(device_type=device):  \n",
    "#             atomic_contributions = painn(\n",
    "#                 atoms=batch.z,\n",
    "#                 atom_positions=batch.pos,\n",
    "#                 graph_indexes=batch.batch,\n",
    "#             )\n",
    "#             preds = post_processing(\n",
    "#                 atoms=batch.z,\n",
    "#                 graph_indexes=batch.batch,\n",
    "#                 atomic_contributions=atomic_contributions,\n",
    "#             )\n",
    "#             loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "#             loss = loss_step / len(batch.y)\n",
    "\n",
    "#         # Backpropagation with mixed precision\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         loss_epoch += loss_step.detach().item()\n",
    "\n",
    "#     # Average the loss per epoch\n",
    "#     loss_epoch /= len(dm.train_dataloader())\n",
    "#     pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
    "\n",
    "# # Test loop\n",
    "# mae = 0.0\n",
    "# painn.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch in dm.test_dataloader():\n",
    "#         batch = batch.to(device)\n",
    "\n",
    "#         atomic_contributions = painn(\n",
    "#             atoms=batch.z,\n",
    "#             atom_positions=batch.pos,\n",
    "#             graph_indexes=batch.batch,\n",
    "#         )\n",
    "#         preds = post_processing(\n",
    "#             atoms=batch.z,\n",
    "#             graph_indexes=batch.batch,\n",
    "#             atomic_contributions=atomic_contributions,\n",
    "#         )\n",
    "#         mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "# mae /= len(dm.data_test)\n",
    "# unit_conversion = dm.unit_conversion[args.target]\n",
    "# print(f'Test MAE: {unit_conversion(mae):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14952, 1])\n",
      "torch.Size([14952])\n",
      "14952\n",
      "14952\n"
     ]
    }
   ],
   "source": [
    "print(atomic_contributions.shape)\n",
    "print(batch.z.shape)\n",
    "\n",
    "print(batch.batch.shape[0])\n",
    "print(atomic_contributions.shape[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
