{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QM9DataModule\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PaiNN, AtomwisePostProcessing\n",
      "File \u001b[1;32mc:\\Users\\WenhaoLi\\Documents\\study\\new master\\02456 Deep learning\\PaiNN\\src\\data\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqm9\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QM9DataModule\n",
      "File \u001b[1;32mc:\\Users\\WenhaoLi\\Documents\\study\\new master\\02456 Deep learning\\PaiNN\\src\\data\\qm9.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QM9\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic example of how to train the PaiNN model to predict the QM9 property\n",
    "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
    "properties) is computed as a sum of atomic contributions.\n",
    "\"\"\"\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from src.data import QM9DataModule\n",
    "from pytorch_lightning import seed_everything\n",
    "from src.models import PaiNN, AtomwisePostProcessing\n",
    "\n",
    "\n",
    "def cli():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0)\n",
    "\n",
    "    # Data\n",
    "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
    "    parser.add_argument('--data_dir', default='data/', type=str)\n",
    "    parser.add_argument('--batch_size_train', default=100, type=int)\n",
    "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
    "    parser.add_argument('--subset_size', default=None, type=int)\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument('--num_message_passing_layers', default=3, type=int)\n",
    "    parser.add_argument('--num_features', default=128, type=int)\n",
    "    parser.add_argument('--num_outputs', default=1, type=int)\n",
    "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
    "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
    "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--num_epochs', default=1000, type=int)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = cli()\n",
    "    seed_everything(args.seed)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    dm = QM9DataModule(\n",
    "        target=args.target,\n",
    "        data_dir=args.data_dir,\n",
    "        batch_size_train=args.batch_size_train,\n",
    "        batch_size_inference=args.batch_size_inference,\n",
    "        num_workers=args.num_workers,\n",
    "        splits=args.splits,\n",
    "        seed=args.seed,\n",
    "        subset_size=args.subset_size,\n",
    "    )\n",
    "    dm.prepare_data()\n",
    "    dm.setup()\n",
    "    y_mean, y_std, atom_refs = dm.get_target_stats(\n",
    "        remove_atom_refs=True, divide_by_atoms=True\n",
    "    )\n",
    "\n",
    "    painn = PaiNN(\n",
    "        num_message_passing_layers=args.num_message_passing_layers,\n",
    "        num_features=args.num_features,\n",
    "        num_outputs=args.num_outputs, \n",
    "        num_rbf_features=args.num_rbf_features,\n",
    "        num_unique_atoms=args.num_unique_atoms,\n",
    "        cutoff_dist=args.cutoff_dist,\n",
    "    )\n",
    "    post_processing = AtomwisePostProcessing(\n",
    "        args.num_outputs, y_mean, y_std, atom_refs\n",
    "    )\n",
    "\n",
    "    painn.to(device)\n",
    "    post_processing.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        painn.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "\n",
    "    painn.train()\n",
    "    pbar = trange(args.num_epochs)\n",
    "    for epoch in pbar:\n",
    "\n",
    "        loss_epoch = 0.\n",
    "        for batch in dm.train_dataloader():\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            atomic_contributions = painn(\n",
    "                atoms=batch.z,\n",
    "                atom_positions=batch.pos,\n",
    "                graph_indexes=batch.batch\n",
    "            )\n",
    "            preds = post_processing(\n",
    "                atoms=batch.z,\n",
    "                graph_indexes=batch.batch,\n",
    "                atomic_contributions=atomic_contributions,\n",
    "            )\n",
    "            loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "            loss = loss_step / len(batch.y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss_step.detach().item()\n",
    "        loss_epoch /= len(dm.data_train)\n",
    "        pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
    "\n",
    "    mae = 0\n",
    "    painn.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dm.test_dataloader():\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            atomic_contributions = painn(\n",
    "                atoms=batch.z,\n",
    "                atom_positions=batch.pos,\n",
    "                graph_indexes=batch.batch,\n",
    "            )\n",
    "            preds = post_processing(\n",
    "                atoms=batch.z,\n",
    "                graph_indexes=batch.batch,\n",
    "                atomic_contributions=atomic_contributions,\n",
    "            )\n",
    "            mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "    \n",
    "    mae /= len(dm.data_test)\n",
    "    unit_conversion = dm.unit_conversion[args.target]\n",
    "    print(f'Test MAE: {unit_conversion(mae):.3f}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldtu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
