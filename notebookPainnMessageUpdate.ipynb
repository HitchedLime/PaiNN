{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02456 Molecular Property Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic example of how to train the PaiNN model to predict the QM9 property\n",
    "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
    "properties) is computed as a sum of atomic contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QM9 Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import Optional, List, Union, Tuple\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "\n",
    "class GetTarget(BaseTransform):\n",
    "    def __init__(self, target: Optional[int] = None) -> None:\n",
    "        self.target = [target]\n",
    "\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        if self.target is not None:\n",
    "            data.y = data.y[:, self.target]\n",
    "        return data\n",
    "\n",
    "\n",
    "class QM9DataModule(pl.LightningDataModule):\n",
    "\n",
    "    target_types = ['atomwise' for _ in range(19)]\n",
    "    target_types[0] = 'dipole_moment'\n",
    "    target_types[5] = 'electronic_spatial_extent'\n",
    "\n",
    "    # Specify unit conversions (eV to meV).\n",
    "    unit_conversion = {\n",
    "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
    "        else (lambda t: t)\n",
    "        for i in range(19)\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: int = 7,\n",
    "        data_dir: str = 'data/',\n",
    "        batch_size_train: int = 100,\n",
    "        batch_size_inference: int = 1000,\n",
    "        num_workers: int = 0,\n",
    "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
    "        seed: int = 0,\n",
    "        subset_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.batch_size_inference = batch_size_inference\n",
    "        self.num_workers = num_workers\n",
    "        self.splits = splits\n",
    "        self.seed = seed\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download data\n",
    "        QM9(root=self.data_dir)\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        indices = torch.randperm(len(dataset))\n",
    "        dataset = dataset[indices]\n",
    "\n",
    "        # Subset dataset\n",
    "        if self.subset_size is not None:\n",
    "            dataset = dataset[:self.subset_size]\n",
    "        \n",
    "        # Split dataset\n",
    "        if all([type(split) == int for split in self.splits]):\n",
    "            split_sizes = self.splits\n",
    "        elif all([type(split) == float for split in self.splits]):\n",
    "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
    "\n",
    "        split_idx = np.cumsum(split_sizes)\n",
    "        self.data_train = dataset[:split_idx[0]]\n",
    "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
    "        self.data_test = dataset[split_idx[1]:]\n",
    "\n",
    "\n",
    "    def get_target_stats(\n",
    "        self,\n",
    "        remove_atom_refs: bool = True,\n",
    "        divide_by_atoms: bool = True\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        atom_refs = self.data_train.atomref(self.target)\n",
    "\n",
    "        ys = list()\n",
    "        for batch in self.train_dataloader(shuffle=False):\n",
    "            y = batch.y.clone()\n",
    "            if remove_atom_refs and atom_refs is not None:\n",
    "                y.index_add_(\n",
    "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
    "                )\n",
    "            if divide_by_atoms:\n",
    "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
    "                y = y / num_atoms.unsqueeze(-1)\n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.cat(ys, dim=0)\n",
    "        return y.mean(), y.std(), atom_refs\n",
    "\n",
    "\n",
    "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.batch_size_train,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AtomwisePostProcessing(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-processing for (QM9) properties that are predicted as sums of atomic\n",
    "    contributions.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs: int,\n",
    "        mean: torch.FloatTensor,\n",
    "        std: torch.FloatTensor,\n",
    "        atom_refs: torch.FloatTensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_outputs: Integer with the number of model outputs. In most\n",
    "                cases 1.\n",
    "            mean: torch.FloatTensor with mean value to shift atomwise\n",
    "                contributions by.\n",
    "            std: torch.FloatTensor with standard deviation to scale atomwise\n",
    "                contributions by.\n",
    "            atom_refs: torch.FloatTensor of size [num_atom_types, 1] with\n",
    "                atomic reference values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.register_buffer('scale', std)\n",
    "        self.register_buffer('shift', mean)\n",
    "        self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_contributions: torch.FloatTensor,\n",
    "        atoms: torch.LongTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Atomwise post-processing operations and atomic sum.\n",
    "\n",
    "        Args:\n",
    "            atomic_contributions: torch.FloatTensor of size [num_nodes,\n",
    "                num_outputs] with each node's contribution to the overall graph\n",
    "                prediction, i.e., each atom's contribution to the overall\n",
    "                molecular property prediction.\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FLoatTensor of size [num_graphs, num_outputs] with\n",
    "            predictions for each graph (molecule).\n",
    "        \"\"\"\n",
    "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
    "\n",
    "        atomic_contributions = atomic_contributions*self.scale + self.shift\n",
    "        atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
    "\n",
    "        # Sum contributions for each graph\n",
    "        output_per_graph = torch.zeros(\n",
    "            (num_graphs, self.num_outputs),\n",
    "            device=atomic_contributions.device,\n",
    "        )\n",
    "        output_per_graph.index_add_(\n",
    "            dim=0,\n",
    "            index=graph_indexes,\n",
    "            source=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        return output_per_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Linear, SiLU\n",
    "import torch_scatter\n",
    "\n",
    "class Message(nn.Module):\n",
    "    def __init__(self, Ls=None, Lrbf=None, nRbf=20, nF=128):\n",
    "        super(Message, self).__init__()\n",
    "        self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "            Linear(nF, nF),\n",
    "            SiLU(),\n",
    "            Linear(nF, 3*nF),\n",
    "        )\n",
    "        self.Lrbf = Lrbf if Lrbf is not None else Linear(nRbf, 3*nF)\n",
    "\n",
    "    def fCut(self, rij_norm, rCut):\n",
    "        f_cut = 0.5 * (torch.cos(torch.pi * rij_norm / rCut) + 1)\n",
    "        f_cut[rij_norm > rCut] = 0 \n",
    "        return f_cut\n",
    "\n",
    "    def fRBF(self, rij_norm, rCut, nRbf=20):\n",
    "        Trbf = torch.arange(1, nRbf + 1, device=rij_norm.device).float()\n",
    "        rij_norm = rij_norm.unsqueeze(-1)  \n",
    "        RBF = torch.sin(Trbf * torch.pi * rij_norm / rCut) / (rij_norm + 1e-8)\n",
    "        return RBF\n",
    "\n",
    "    def forward(self, vj, sj, rij_vec, eij, rCut=5.0, nRbf=20):\n",
    "        rij_norm = torch.norm(rij_vec, dim=-1)\n",
    "        rij_hat =  rij_vec / (rij_norm.unsqueeze(-1) + 1e-8)\n",
    "\n",
    "        RBF = self.fRBF(rij_norm, rCut, nRbf)\n",
    "        T_RBF = self.Lrbf(RBF)\n",
    "        Ws = T_RBF * self.fCut(rij_norm,5.0).unsqueeze(-1) \n",
    "\n",
    "        phi = self.Ls(sj)\n",
    "        phiW = phi * Ws\n",
    "\n",
    "        # SPLIT1 = phiW[:,0:128]\n",
    "        # SPLIT2 = phiW[:,128:256]\n",
    "        # SPLIT3 = phiW[:,256:]\n",
    "        \n",
    "        \n",
    "        SPLIT1,SPLIT2,SPLIT3         = torch.split( phiW, 128, dim=1)\n",
    "\n",
    "        phiWvv = vj * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "        phiWvs = SPLIT3.unsqueeze(-1) * rij_hat.unsqueeze(1)\n",
    "        \n",
    "        d_vim = torch_scatter.scatter_sum((phiWvv + phiWvs), eij[1], dim=0)\n",
    "        d_sim = torch_scatter.scatter_sum(SPLIT2, eij[1], dim=0)\n",
    "        return d_vim, d_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Update(nn.Module):\n",
    "    def __init__(self, Luu=None, Luv=None, Ls=None):\n",
    "        super(Update, self).__init__()\n",
    "        self.Luu = Luu if Luu is not None else Linear(3, 3, False)\n",
    "        self.Luv = Luv if Luv is not None else Linear(3, 3, False)\n",
    "        \n",
    "        self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "            Linear(in_features=256, out_features=128),\n",
    "            SiLU(),\n",
    "            Linear(in_features=128, out_features=384),\n",
    "        )\n",
    "\n",
    "    def forward(self, vi, si):\n",
    "        Uvi = self.Luu(vi) \n",
    "        Vvi = self.Luv(vi)\n",
    "\n",
    "        V_norm = torch.norm(Vvi,dim=-1)\n",
    "        STACK = torch.hstack([V_norm, si])\n",
    "\n",
    "        SP = torch.sum(Uvi * Vvi, dim=-1) \n",
    "\n",
    "        SPLIT = self.Ls(STACK)\n",
    "        # SPLIT1 = SPLIT[:, 0:128]\n",
    "        # SPLIT2 = SPLIT[:, 128:256]\n",
    "        # SPLIT3 = SPLIT[:, 256:]\n",
    "        \n",
    "        SPLIT1,SPLIT2,SPLIT3         = torch.split(SPLIT, 128, dim=1)\n",
    "        \n",
    "        \n",
    "        d_viu = Uvi * SPLIT1.unsqueeze(-1).repeat(1, 1, 3)\n",
    "        d_siu = SP * SPLIT2 + SPLIT3\n",
    "\n",
    "        return d_viu, d_siu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import radius_graph\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, Lm, Lu,\n",
    "        num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_message_passing_layers = num_message_passing_layers\n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.num_unique_atoms = num_unique_atoms\n",
    "        self.cutoff_dist = cutoff_dist\n",
    "\n",
    "        self.zi = nn.Embedding(num_unique_atoms, num_features)\n",
    "\n",
    "        self.Lm = Lm\n",
    "        self.Lu = Lu\n",
    "\n",
    "        self.Lr = nn.Sequential(\n",
    "            Linear(in_features=128, out_features=64),\n",
    "            SiLU(),\n",
    "            Linear(in_features=64, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atoms: torch.LongTensor,\n",
    "        atom_positions: torch.FloatTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        si = self.zi(atoms)\n",
    "        eij = radius_graph(atom_positions, r=self.cutoff_dist, batch=graph_indexes)\n",
    "        sj = si[eij[0]]\n",
    "        vi = torch.zeros_like(si).unsqueeze(-1).repeat(1, 1, 3)\n",
    "        vj = vi[eij[0]]\n",
    "        rij_vec = atom_positions[eij[0]] - atom_positions[eij[1]]\n",
    "        for _ in range(self.num_message_passing_layers):\n",
    "            d_vim, d_sim = self.Lm(vj, sj, rij_vec, eij)\n",
    "            vi = vi + d_vim\n",
    "            si = si + d_sim\n",
    "\n",
    "            d_viu, d_siu = self.Lu(vi, si)\n",
    "\n",
    "            vi = vi + d_viu\n",
    "            si = si + d_siu\n",
    "        \n",
    "        Sigma = self.Lr(si)\n",
    "\n",
    "        return Sigma\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli(args: list = []):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0)\n",
    "\n",
    "    # Data\n",
    "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
    "    parser.add_argument('--data_dir', default='data/', type=str)\n",
    "    parser.add_argument('--batch_size_train', default=100, type=int)\n",
    "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
    "    parser.add_argument('--subset_size', default=None, type=int)\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument('--num_message_passing_layers', default=3, type=int)\n",
    "    parser.add_argument('--num_features', default=128, type=int)\n",
    "    parser.add_argument('--num_outputs', default=1, type=int)\n",
    "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
    "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
    "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--num_epochs', default=1000, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = model.state_dict()\n",
    "        elif val_loss >= self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = model.state_dict()\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_10032\\3057224435.py:108: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1823.)\n",
      "  return y.mean(), y.std(), atom_refs\n",
      " 40%|████      | 2/5 [00:00<00:00, 88.25it/s, Train loss: 0.000e+00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.000\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Test MAE: 0.000\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args = [] # Specify non-default arguments in this list\n",
    "args = cli(args)\n",
    "seed_everything(args.seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "print(device_name)\n",
    "\n",
    "dm = QM9DataModule(\n",
    "    target=args.target,\n",
    "    data_dir=args.data_dir,\n",
    "    batch_size_train=args.batch_size_train,\n",
    "    batch_size_inference=args.batch_size_inference,\n",
    "    num_workers=args.num_workers,\n",
    "    splits=args.splits,\n",
    "    seed=args.seed,\n",
    "    subset_size=args.subset_size,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "y_mean, y_std, atom_refs = dm.get_target_stats(\n",
    "    remove_atom_refs=True, divide_by_atoms=True\n",
    ")\n",
    "\n",
    "painn = PaiNN(\n",
    "    Lm=Message(),\n",
    "    Lu=Update(),\n",
    "    num_message_passing_layers=args.num_message_passing_layers,\n",
    "    num_features=args.num_features,\n",
    "    num_outputs=args.num_outputs, \n",
    "    num_rbf_features=args.num_rbf_features,\n",
    "    num_unique_atoms=args.num_unique_atoms,\n",
    "    cutoff_dist=args.cutoff_dist,\n",
    ")\n",
    "post_processing = AtomwisePostProcessing(\n",
    "    args.num_outputs, y_mean, y_std, atom_refs\n",
    ")\n",
    "\n",
    "painn.to(device)\n",
    "post_processing.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    painn.parameters(),\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, verbose=True)\n",
    "\n",
    "\n",
    "painn.train()\n",
    "pbar = trange(args.num_epochs)\n",
    "for epoch in pbar:\n",
    "\n",
    "    loss_epoch = 0.\n",
    "    for batch in dm.train_dataloader():\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "        loss = loss_step / len(batch.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += 0*loss_step.detach().item()\n",
    "    loss_epoch /= len(dm.data_train)\n",
    "    pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
    "\n",
    "    mae = 0\n",
    "    painn.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dm.test_dataloader():\n",
    "          \n",
    "            batch = batch.to(device)\n",
    "\n",
    "            atomic_contributions = painn(\n",
    "                atoms=batch.z,\n",
    "                atom_positions=batch.pos,\n",
    "                graph_indexes=batch.batch,\n",
    "            )\n",
    "            preds = post_processing(\n",
    "                atoms=batch.z,\n",
    "                graph_indexes=batch.batch,\n",
    "                atomic_contributions=atomic_contributions,\n",
    "            )\n",
    "            mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "        mae =0\n",
    "        early_stopping(mae, painn)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            painn.load_state_dict(early_stopping.best_model)\n",
    "            break\n",
    "\n",
    "        mae /= len(dm.data_test)\n",
    "        unit_conversion = dm.unit_conversion[args.target]\n",
    "        print(f'Test MAE: {unit_conversion(mae):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
