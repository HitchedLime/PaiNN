{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02456 Molecular Property Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic example of how to train the PaiNN model to predict the QM9 property\n",
    "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
    "properties) is computed as a sum of atomic contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QM9 Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import Optional, List, Union, Tuple\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "\n",
    "class GetTarget(BaseTransform):\n",
    "    def __init__(self, target: Optional[int] = None) -> None:\n",
    "        self.target = [target]\n",
    "\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        if self.target is not None:\n",
    "            data.y = data.y[:, self.target]\n",
    "        return data\n",
    "\n",
    "\n",
    "class QM9DataModule(pl.LightningDataModule):\n",
    "\n",
    "    target_types = ['atomwise' for _ in range(19)]\n",
    "    target_types[0] = 'dipole_moment'\n",
    "    target_types[5] = 'electronic_spatial_extent'\n",
    "\n",
    "    # Specify unit conversions (eV to meV).\n",
    "    unit_conversion = {\n",
    "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
    "        else (lambda t: t)\n",
    "        for i in range(19)\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: int = 7,\n",
    "        data_dir: str = 'data/',\n",
    "        batch_size_train: int = 100,\n",
    "        batch_size_inference: int = 1000,\n",
    "        num_workers: int = 0,\n",
    "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
    "        seed: int = 0,\n",
    "        subset_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.batch_size_inference = batch_size_inference\n",
    "        self.num_workers = num_workers\n",
    "        self.splits = splits\n",
    "        self.seed = seed\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download data\n",
    "        QM9(root=self.data_dir)\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        dataset = dataset[rng.permutation(len(dataset))]\n",
    "\n",
    "        # Subset dataset\n",
    "        if self.subset_size is not None:\n",
    "            dataset = dataset[:self.subset_size]\n",
    "        \n",
    "        # Split dataset\n",
    "        if all([type(split) == int for split in self.splits]):\n",
    "            split_sizes = self.splits\n",
    "        elif all([type(split) == float for split in self.splits]):\n",
    "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
    "\n",
    "        split_idx = np.cumsum(split_sizes)\n",
    "        self.data_train = dataset[:split_idx[0]]\n",
    "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
    "        self.data_test = dataset[split_idx[1]:]\n",
    "\n",
    "\n",
    "    def get_target_stats(\n",
    "        self,\n",
    "        remove_atom_refs: bool = True,\n",
    "        divide_by_atoms: bool = True\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        atom_refs = self.data_train.atomref(self.target)\n",
    "\n",
    "        ys = list()\n",
    "        for batch in self.train_dataloader(shuffle=False):\n",
    "            y = batch.y.clone()\n",
    "            if remove_atom_refs and atom_refs is not None:\n",
    "                y.index_add_(\n",
    "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
    "                )\n",
    "            if divide_by_atoms:\n",
    "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
    "                y = y / num_atoms.unsqueeze(-1)\n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.cat(ys, dim=0)\n",
    "        return y.mean(), y.std(), atom_refs\n",
    "\n",
    "\n",
    "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.batch_size_train,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AtomwisePostProcessing(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-processing for (QM9) properties that are predicted as sums of atomic\n",
    "    contributions.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs: int,\n",
    "        mean: torch.FloatTensor,\n",
    "        std: torch.FloatTensor,\n",
    "        atom_refs: torch.FloatTensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_outputs: Integer with the number of model outputs. In most\n",
    "                cases 1.\n",
    "            mean: torch.FloatTensor with mean value to shift atomwise\n",
    "                contributions by.\n",
    "            std: torch.FloatTensor with standard deviation to scale atomwise\n",
    "                contributions by.\n",
    "            atom_refs: torch.FloatTensor of size [num_atom_types, 1] with\n",
    "                atomic reference values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.register_buffer('scale', std)\n",
    "        self.register_buffer('shift', mean)\n",
    "        self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_contributions: torch.FloatTensor,\n",
    "        atoms: torch.LongTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Atomwise post-processing operations and atomic sum.\n",
    "\n",
    "        Args:\n",
    "            atomic_contributions: torch.FloatTensor of size [num_nodes,\n",
    "                num_outputs] with each node's contribution to the overall graph\n",
    "                prediction, i.e., each atom's contribution to the overall\n",
    "                molecular property prediction.\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FLoatTensor of size [num_graphs, num_outputs] with\n",
    "            predictions for each graph (molecule).\n",
    "        \"\"\"\n",
    "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
    "\n",
    "        atomic_contributions = atomic_contributions*self.scale + self.shift\n",
    "        atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
    "\n",
    "        # Sum contributions for each graph\n",
    "        output_per_graph = torch.zeros(\n",
    "            (num_graphs, self.num_outputs),\n",
    "            device=atomic_contributions.device,\n",
    "        )\n",
    "        output_per_graph.index_add_(\n",
    "            dim=0,\n",
    "            index=graph_indexes,\n",
    "            source=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        return output_per_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, SiLU\n",
    "from torch_scatter import scatter_sum\n",
    "\n",
    "class Message(nn.Module):\n",
    "    def __init__(self, Ls=None, Lrbf=None, Lsplit=None, nRbf=20, nF=128):\n",
    "        super(Message, self).__init__()\n",
    "        self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "            Linear(nF, nF, False),\n",
    "            SiLU(),\n",
    "            Linear(nF, 3*nF, False),\n",
    "        )\n",
    "        self.Lrbf = Lrbf if Lrbf is not None else Linear(nRbf, 3*nF, False)\n",
    "        self.Lsplit = Lsplit if Lsplit is not None else Linear(3*nF, nF, False)\n",
    "\n",
    "    def fCut(self, rij_norm, rCut):\n",
    "        f_cut = 0.5 * (torch.cos(torch.pi * rij_norm / rCut) + 1)\n",
    "        f_cut[rij_norm > rCut] = 0 \n",
    "        return f_cut\n",
    "\n",
    "    def fRBF(self, rij_norm, rCut, nRbf=20):\n",
    "        Trbf = torch.arange(1, nRbf + 1, device=rij_norm.device).float()\n",
    "        rij_norm = rij_norm.unsqueeze(-1)  \n",
    "        RBF = torch.sin(Trbf * torch.pi * rij_norm / rCut) / (rij_norm + 1e-8)\n",
    "        return RBF\n",
    "\n",
    "    def forward(self, vj, sj, rij_vec, eij, rCut=5.0, nRbf=20):\n",
    "        rij_norm = torch.norm(rij_vec, dim=-1)\n",
    "        rij_hat =  rij_vec / (rij_norm.unsqueeze(-1) + 1e-8)\n",
    "\n",
    "        RBF = self.fRBF(rij_norm, rCut, nRbf)\n",
    "        T_RBF = self.Lrbf(RBF)\n",
    "        Ws = T_RBF * self.fCut(rij_norm,5.0).unsqueeze(-1) \n",
    "\n",
    "        phi = self.Ls(sj)\n",
    "        phiW = phi * Ws\n",
    "\n",
    "        SPLIT = self.Lsplit(phiW)\n",
    "\n",
    "        phiWvv = vj * SPLIT.unsqueeze(-1).repeat(1, 1, 3)\n",
    "        phiWvs = SPLIT.unsqueeze(-1) * rij_hat.unsqueeze(1)\n",
    "        \n",
    "        d_vim = scatter_sum((phiWvv + phiWvs), eij[0], dim=0)\n",
    "        d_sim = scatter_sum(SPLIT, eij[0], dim=0)\n",
    "        return d_vim, d_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Update(nn.Module):\n",
    "    def __init__(self, Luu=None, Luv=None, Ls=None):\n",
    "        super(Update, self).__init__()\n",
    "        self.Luu = Luu if Luu is not None else Linear(3, 3, False)\n",
    "        self.Luv = Luv if Luv is not None else Linear(3, 3, False)\n",
    "        \n",
    "        self.Ls = Ls if Ls is not None else nn.Sequential(\n",
    "            Linear(in_features=256, out_features=128, bias=False),\n",
    "            SiLU(),\n",
    "            Linear(in_features=128, out_features=384, bias=False),\n",
    "            Linear(in_features=384, out_features=128, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, vj, sj, eij):\n",
    "        Uvj = self.Luu(vj) \n",
    "        Vvj = self.Luv(vj)\n",
    "\n",
    "        V_norm = torch.norm(Vvj,dim=-1)\n",
    "        STACK = torch.hstack([V_norm, sj])\n",
    "\n",
    "        SP = torch.sum(Uvj * Vvj, dim=-1) \n",
    "\n",
    "        SPLIT = self.Ls(STACK)\n",
    "\n",
    "        d_viu = scatter_sum((Uvj * SPLIT.unsqueeze(-1).repeat(1, 1, 3)), eij[0], dim=0)\n",
    "        d_siu = scatter_sum((SP * SPLIT[eij[0]] + SPLIT[eij[0]]), eij[0], dim=0)\n",
    "\n",
    "        return d_viu, d_siu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_cluster import radius_graph\n",
    "\n",
    "class PaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, Lm, Lu,\n",
    "        num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_message_passing_layers: Number of message passing layers in\n",
    "                the PaiNN model.\n",
    "            num_features: Size of the node embeddings (scalar features) and\n",
    "                vector features.\n",
    "            num_outputs: Number of model outputs. In most cases 1.\n",
    "            num_rbf_features: Number of radial basis functions to represent\n",
    "                distances.\n",
    "            num_unique_atoms: Number of unique atoms in the data that we want\n",
    "                to learn embeddings for.\n",
    "            cutoff_dist: Euclidean distance threshold for determining whether \n",
    "                two nodes (atoms) are neighbours.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #raise NotImplementedError\n",
    "        self.num_message_passing_layers = num_message_passing_layers\n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.num_unique_atoms = num_unique_atoms\n",
    "        self.cutoff_dist = cutoff_dist\n",
    "\n",
    "        self.zi = nn.Embedding(num_unique_atoms, num_features)\n",
    "\n",
    "        self.Lm = Lm\n",
    "        self.Lu = Lu\n",
    "\n",
    "        self.Lr = nn.Sequential(\n",
    "            Linear(in_features=128, out_features=64, bias=False),\n",
    "            SiLU(),\n",
    "            Linear(in_features=64, out_features=1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atoms: torch.LongTensor,\n",
    "        atom_positions: torch.FloatTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        si = self.zi(atoms)\n",
    "        eij = radius_graph(atom_positions, r=self.cutoff_dist, batch=graph_indexes)\n",
    "        sj = si[eij[1]]\n",
    "        vi = torch.zeros_like(si).unsqueeze(-1).repeat(1, 1, 3)\n",
    "        vj = vi[eij[1]]\n",
    "        rij_vec = atom_positions[eij[0]] - atom_positions[eij[1]]\n",
    "        for _ in range(self.num_message_passing_layers):\n",
    "            d_vim, d_sim = self.Lm(vj, sj, rij_vec, eij)\n",
    "            vi += d_vim\n",
    "            si += d_sim\n",
    "\n",
    "            sj = si[eij[1]]\n",
    "            vj = vi[eij[1]]\n",
    "\n",
    "            d_viu, d_siu = self.Lu(vj, sj, eij)\n",
    "\n",
    "            vi += d_viu\n",
    "            si += d_siu\n",
    "        \n",
    "        Sigma = self.Lr(si)\n",
    "\n",
    "        return Sigma\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli(args: list = []):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0)\n",
    "\n",
    "    # Data\n",
    "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
    "    parser.add_argument('--data_dir', default='data/', type=str)\n",
    "    parser.add_argument('--batch_size_train', default=100, type=int)\n",
    "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
    "    parser.add_argument('--subset_size', default=None, type=int)\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument('--num_message_passing_layers', default=3, type=int)\n",
    "    parser.add_argument('--num_features', default=128, type=int)\n",
    "    parser.add_argument('--num_outputs', default=1, type=int)\n",
    "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
    "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
    "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--num_epochs', default=1000, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "  0%|          | 1/1000 [01:39<27:39:55, 99.70s/it, Train loss: 2.270e+00]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     68\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 70\u001b[0m     loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m loss_epoch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dm\u001b[38;5;241m.\u001b[39mdata_train)\n\u001b[0;32m     72\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix_str(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_epoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = [] # Specify non-default arguments in this list\n",
    "args = cli(args)\n",
    "seed_everything(args.seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dm = QM9DataModule(\n",
    "    target=args.target,\n",
    "    data_dir=args.data_dir,\n",
    "    batch_size_train=args.batch_size_train,\n",
    "    batch_size_inference=args.batch_size_inference,\n",
    "    num_workers=args.num_workers,\n",
    "    splits=args.splits,\n",
    "    seed=args.seed,\n",
    "    subset_size=args.subset_size,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "y_mean, y_std, atom_refs = dm.get_target_stats(\n",
    "    remove_atom_refs=True, divide_by_atoms=True\n",
    ")\n",
    "\n",
    "painn = PaiNN(\n",
    "    Lm=Message(),\n",
    "    Lu=Update(),\n",
    "    num_message_passing_layers=args.num_message_passing_layers,\n",
    "    num_features=args.num_features,\n",
    "    num_outputs=args.num_outputs, \n",
    "    num_rbf_features=args.num_rbf_features,\n",
    "    num_unique_atoms=args.num_unique_atoms,\n",
    "    cutoff_dist=args.cutoff_dist,\n",
    ")\n",
    "post_processing = AtomwisePostProcessing(\n",
    "    args.num_outputs, y_mean, y_std, atom_refs\n",
    ")\n",
    "\n",
    "painn.to(device)\n",
    "post_processing.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    painn.parameters(),\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "painn.train()\n",
    "pbar = trange(args.num_epochs)\n",
    "for epoch in pbar:\n",
    "\n",
    "    loss_epoch = 0.\n",
    "    for batch in dm.train_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "        loss = loss_step / len(batch.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss_step.detach().item()\n",
    "    loss_epoch /= len(dm.data_train)\n",
    "    pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
    "\n",
    "mae = 0\n",
    "painn.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dm.test_dataloader():\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch,\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "mae /= len(dm.data_test)\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "print(f'Test MAE: {unit_conversion(mae):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
